{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This file is for the purposes of explaining the code flow of training classifier for execution it is advised to use .py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Starting the code with importing all the required libraries. Since it is image based approach\n",
    "    and keras is used as prefered library for this task. the importing contains cv2 and keras as\n",
    "    points of interest remaining imports are for the purpose of data handling and accessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import pdb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The below code snippet is used to restrict the usage of GPU to a certain ratio.\n",
    "    This is under quotes but, if required remove the quotes and execute the cell and its effect\n",
    "    can be seen during training when tensor is created on to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# This part of the code is used to restrict the GPU usage.\n",
    "import tensorflow as tf \n",
    "from keras.backend.tensorflow_backend import set_session \n",
    "config = tf.ConfigProto() \n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3 \n",
    "set_session(tf.Session(config=config))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Below funcation named data_spinner is a data processor which takes image paths of dataset\n",
    "    and once the image is read its is nornalizzed and is made neural network ready. And after\n",
    "    all the processing is done it spins out the data and label as output for the training operation.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_spinner(data_path):\n",
    "    random.shuffle(data_path)\n",
    "    Y = []\n",
    "    X = []\n",
    "    for img_path in data_path:\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, (img_rows, img_cols))\n",
    "        img1 = np.array(img, dtype='float')/255.0\n",
    "        X.append(img1)\n",
    "        name = img_path.split(\"/\")\n",
    "        if 'recepit' in name:\n",
    "            Y.append([1, 0])\n",
    "        if 'non-recepit' in name:\n",
    "            Y.append([0, 1])\n",
    "    Y = np.array(Y)\n",
    "    # pdb.set_trace()\n",
    "    X = np.array(X)\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The Create Model Funcation is key funcation as it is responsible for creating \n",
    "    the model architecture. On calling this funcation it returns a model which is used for\n",
    "    training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_rows, img_cols, 3)\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    base_model = VGG16(weights='imagenet', include_top=False)\n",
    "    model = base_model.output\n",
    "    model = GlobalAveragePooling2D()(model)\n",
    "    x = Dense(1024, activation='relu')(model)\n",
    "    predictions = Dense(2, activation='softmax')(x)\n",
    "    model = Model(input=base_model.input, output=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The Train funcation where all the magic happens. When model is compiled it takes a\n",
    "    Adam optimizer funcation as optimization funcation and tries to minimize \n",
    "    the 'categorical_crossentropy' loss for achiving a certain metric 'accuracy' in this case.\n",
    "    This compilation when fit funcation is called on to the model sincce it is a itreative process \n",
    "    it takes arguments such as batch size to set how many images to be processed as a time,\n",
    "    epoches to say how many times the itreation will run. And here itself we can create a \n",
    "    validation split. If required this can be created exclusivly and then called inside.\n",
    "    And finaly when all the itreations are completed the model is save which is then used \n",
    "    for runing the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model = create_model()\n",
    "    model.compile(optimizer=Adam(lr=0.0001, beta_1=0.9, beta_2=0.999),\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X, Y, batch_size=16, epochs=30, verbose=1,\n",
    "              shuffle=True, validation_split=0.3)\n",
    "    model.save_weights('./model/recpit.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Here all the necessary funcation calls are made in order to make all the things happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = \"/home/akshay/Recpit_Classification/data/*/*\"\n",
    "img_rows, img_cols = 224, 224\n",
    "data_path = glob.glob(paths)\n",
    "print(len(data_path))\n",
    "X, Y = data_spinner(data_path)\n",
    "print(X)\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
